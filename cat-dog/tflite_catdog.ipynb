{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "dfec"
      },
      "source": [
        "##### Copyright 2023 Brian Yarbrough."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Tensorflow Lite\n",
        "\n",
        "Previously, we trained a model based on MobileNetV2 to differentiate between cats and dogs.\n",
        "\n",
        "Prior to conducting inference with this model, we will convert the model to Tensorflow Lite.\n",
        "This will have performance advantages on constrained hardware.\n",
        "\n",
        "## Upload the Model\n",
        "\n",
        "First, upload the saved model (`cat-dog-tuned.zip` if you used the previous tutorial).\n",
        "Then unzip the model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archive:  cat-dog-tuned.zip\n",
            "   creating: cat-dog-tuned/\n",
            "  inflating: cat-dog-tuned/keras_metadata.pb  \n",
            "   creating: cat-dog-tuned/variables/\n",
            "  inflating: cat-dog-tuned/variables/variables.index  \n",
            "  inflating: cat-dog-tuned/variables/variables.data-00000-of-00001  \n",
            "   creating: cat-dog-tuned/assets/\n",
            " extracting: cat-dog-tuned/fingerprint.pb  \n",
            "  inflating: cat-dog-tuned/saved_model.pb  \n"
          ]
        }
      ],
      "source": [
        "# Run after uploading the file\n",
        "!unzip cat-dog-tuned.zip"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Convert the saved model\n",
        "\n",
        "We'll use [TFLiteConverter](https://www.tensorflow.org/lite/api_docs/python/tf/lite/TFLiteConverter) to export the model to a single binary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pT8t5BDynwh_",
        "outputId": "0a1e1b26-8e20-401a-ae47-dfec0979c374"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.12.0\n",
            "Help on class TFLiteConverterV2 in module tensorflow.lite.python.lite:\n",
            "\n",
            "class TFLiteConverterV2(TFLiteFrozenGraphConverterV2)\n",
            " |  TFLiteConverterV2(funcs, trackable_obj=None)\n",
            " |  \n",
            " |  Converts a TensorFlow model into TensorFlow Lite model.\n",
            " |  \n",
            " |  Attributes:\n",
            " |    optimizations: Experimental flag, subject to change. Set of optimizations to\n",
            " |      apply. e.g {tf.lite.Optimize.DEFAULT}. (default None, must be None or a\n",
            " |      set of values of type `tf.lite.Optimize`)\n",
            " |    representative_dataset: A generator function used for integer quantization\n",
            " |      where each generated sample has the same order, type and shape as the\n",
            " |      inputs to the model. Usually, this is a small subset of a few hundred\n",
            " |      samples randomly chosen, in no particular order, from the training or\n",
            " |      evaluation dataset. This is an optional attribute, but required for full\n",
            " |      integer quantization, i.e, if `tf.int8` is the only supported type in\n",
            " |      `target_spec.supported_types`. Refer to `tf.lite.RepresentativeDataset`.\n",
            " |      (default None)\n",
            " |    target_spec: Experimental flag, subject to change. Specifications of target\n",
            " |      device, including supported ops set, supported types and a set of user's\n",
            " |      defined TensorFlow operators required in the TensorFlow Lite runtime.\n",
            " |      Refer to `tf.lite.TargetSpec`.\n",
            " |    inference_input_type: Data type of the input layer. Note that integer types\n",
            " |      (tf.int8 and tf.uint8) are currently only supported for post training\n",
            " |      integer quantization and quantization aware training. (default tf.float32,\n",
            " |      must be in {tf.float32, tf.int8, tf.uint8})\n",
            " |    inference_output_type: Data type of the output layer. Note that integer\n",
            " |      types (tf.int8 and tf.uint8) are currently only supported for post\n",
            " |      training integer quantization and quantization aware training. (default\n",
            " |      tf.float32, must be in {tf.float32, tf.int8, tf.uint8})\n",
            " |    allow_custom_ops: Boolean indicating whether to allow custom operations.\n",
            " |      When False, any unknown operation is an error. When True, custom ops are\n",
            " |      created for any op that is unknown. The developer needs to provide these\n",
            " |      to the TensorFlow Lite runtime with a custom resolver. (default False)\n",
            " |    exclude_conversion_metadata: Whether not to embed the conversion metadata\n",
            " |      into the converted model. (default False)\n",
            " |    experimental_new_converter: Experimental flag, subject to change. Enables\n",
            " |      MLIR-based conversion. (default True)\n",
            " |    experimental_new_quantizer: Experimental flag, subject to change. Enables\n",
            " |      MLIR-based quantization conversion instead of Flatbuffer-based conversion.\n",
            " |      (default True)\n",
            " |    experimental_enable_resource_variables: Experimental flag, subject to\n",
            " |      change. Enables\n",
            " |      [resource variables](https://tensorflow.org/guide/migrate/tf1_vs_tf2#resourcevariables_instead_of_referencevariables)\n",
            " |      to be converted by this converter. This is only allowed if the\n",
            " |      from_saved_model interface is used. (default True)\n",
            " |  \n",
            " |  Example usage:\n",
            " |  \n",
            " |  ```python\n",
            " |  # Converting a SavedModel to a TensorFlow Lite model.\n",
            " |    converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)\n",
            " |    tflite_model = converter.convert()\n",
            " |  \n",
            " |  # Converting a tf.Keras model to a TensorFlow Lite model.\n",
            " |  converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
            " |  tflite_model = converter.convert()\n",
            " |  \n",
            " |  # Converting ConcreteFunctions to a TensorFlow Lite model.\n",
            " |  converter = tf.lite.TFLiteConverter.from_concrete_functions([func], model)\n",
            " |  tflite_model = converter.convert()\n",
            " |  \n",
            " |  # Converting a Jax model to a TensorFlow Lite model.\n",
            " |  converter = tf.lite.TFLiteConverter.experimental_from_jax([func], [[\n",
            " |      ('input1', input1), ('input2', input2)]])\n",
            " |  tflite_model = converter.convert()\n",
            " |  ```\n",
            " |  \n",
            " |  Method resolution order:\n",
            " |      TFLiteConverterV2\n",
            " |      TFLiteFrozenGraphConverterV2\n",
            " |      TFLiteConverterBaseV2\n",
            " |      TFLiteConverterBase\n",
            " |      builtins.object\n",
            " |  \n",
            " |  Methods defined here:\n",
            " |  \n",
            " |  __init__(self, funcs, trackable_obj=None)\n",
            " |      Constructor for TFLiteConverter.\n",
            " |      \n",
            " |      Args:\n",
            " |        funcs: List of TensorFlow ConcreteFunctions. The list should not contain\n",
            " |          duplicate elements.\n",
            " |        trackable_obj: tf.AutoTrackable object associated with `funcs`. A\n",
            " |          reference to this object needs to be maintained so that Variables do not\n",
            " |          get garbage collected since functions have a weak reference to\n",
            " |          Variables. This is only required when the tf.AutoTrackable object is not\n",
            " |          maintained by the user (e.g. `from_saved_model`).\n",
            " |  \n",
            " |  convert(self)\n",
            " |      Converts a TensorFlow GraphDef based on instance variables.\n",
            " |      \n",
            " |      Returns:\n",
            " |        The converted data in serialized format.\n",
            " |      \n",
            " |      Raises:\n",
            " |        ValueError:\n",
            " |          No concrete functions is specified.\n",
            " |          Multiple concrete functions are specified.\n",
            " |          Input shape is not specified.\n",
            " |          Invalid quantization parameters.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Class methods defined here:\n",
            " |  \n",
            " |  experimental_from_jax(serving_funcs, inputs) from builtins.type\n",
            " |      Creates a TFLiteConverter object from a Jax model with its inputs.\n",
            " |      \n",
            " |      Args:\n",
            " |        serving_funcs: A array of Jax functions with all the weights applied\n",
            " |          already.\n",
            " |        inputs: A array of Jax input placeholders tuples list, e.g.,\n",
            " |          jnp.zeros(INPUT_SHAPE). Each tuple list should correspond with the\n",
            " |          serving function.\n",
            " |      \n",
            " |      Returns:\n",
            " |        TFLiteConverter object.\n",
            " |  \n",
            " |  from_concrete_functions(funcs, trackable_obj=None) from builtins.type\n",
            " |      Creates a TFLiteConverter object from ConcreteFunctions.\n",
            " |      \n",
            " |      Args:\n",
            " |        funcs: List of TensorFlow ConcreteFunctions. The list should not contain\n",
            " |          duplicate elements. Currently converter can only convert a single\n",
            " |          ConcreteFunction. Converting multiple functions is under development.\n",
            " |        trackable_obj:   An `AutoTrackable` object (typically `tf.module`)\n",
            " |          associated with `funcs`. A reference to this object needs to be\n",
            " |          maintained so that Variables do not get garbage collected since\n",
            " |          functions have a weak reference to Variables.\n",
            " |      \n",
            " |      Returns:\n",
            " |        TFLiteConverter object.\n",
            " |      \n",
            " |      Raises:\n",
            " |        Invalid input type.\n",
            " |  \n",
            " |  from_keras_model(model) from builtins.type\n",
            " |      Creates a TFLiteConverter object from a Keras model.\n",
            " |      \n",
            " |      Args:\n",
            " |        model: tf.Keras.Model\n",
            " |      \n",
            " |      Returns:\n",
            " |        TFLiteConverter object.\n",
            " |  \n",
            " |  from_saved_model(saved_model_dir, signature_keys=None, tags=None) from builtins.type\n",
            " |      Creates a TFLiteConverter object from a SavedModel directory.\n",
            " |      \n",
            " |      Args:\n",
            " |        saved_model_dir: SavedModel directory to convert.\n",
            " |        signature_keys: List of keys identifying SignatureDef containing inputs\n",
            " |          and outputs. Elements should not be duplicated. By default the\n",
            " |          `signatures` attribute of the MetaGraphdef is used. (default\n",
            " |          saved_model.signatures)\n",
            " |        tags: Set of tags identifying the MetaGraphDef within the SavedModel to\n",
            " |          analyze. All tags in the tag set must be present. (default\n",
            " |          {tf.saved_model.SERVING} or {'serve'})\n",
            " |      \n",
            " |      Returns:\n",
            " |        TFLiteConverter object.\n",
            " |      \n",
            " |      Raises:\n",
            " |        Invalid signature keys.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors inherited from TFLiteConverterBase:\n",
            " |  \n",
            " |  __dict__\n",
            " |      dictionary for instance variables (if defined)\n",
            " |  \n",
            " |  __weakref__\n",
            " |      list of weak references to the object (if defined)\n",
            "\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "print(help(tf.lite.TFLiteConverter))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To convert the model we will open it and then follow [the docs](https://www.tensorflow.org/lite/models/convert/convert_models#convert_a_savedmodel_recommended_)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-06-09 11:27:34.253195: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:364] Ignored output_format.\n",
            "2023-06-09 11:27:34.253223: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:367] Ignored drop_control_dependency.\n",
            "2023-06-09 11:27:34.253447: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: cat-dog-tuned\n",
            "2023-06-09 11:27:34.283166: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
            "2023-06-09 11:27:34.283196: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: cat-dog-tuned\n",
            "2023-06-09 11:27:34.386039: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
            "2023-06-09 11:27:34.860630: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: cat-dog-tuned\n",
            "2023-06-09 11:27:35.019635: I tensorflow/cc/saved_model/loader.cc:314] SavedModel load for tags { serve }; Status: success: OK. Took 766189 microseconds.\n"
          ]
        }
      ],
      "source": [
        "saved_model_dir = \"cat-dog-tuned\"  # path to the SavedModel directory\n",
        "tflite_model = \"cat-dog.tflite\" # what to save the converted model as\n",
        "\n",
        "# Open the model\n",
        "converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)\n",
        "# Use Dynamic Range Quantization\n",
        "# https://www.tensorflow.org/lite/performance/post_training_quantization\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "# Convert the model.\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "# Save the model.\n",
        "with open('cat-dog.tflite', 'wb') as f:\n",
        "  f.write(tflite_model)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conduct inference on novel images\n",
        "\n",
        "Now that we've converted the model, let's test it with novel images!\n",
        "We can do this both with the full Keras API, or rely on the Tensorflow Lite runtime - based on our hardware and OS.\n",
        "\n",
        "#### Upload images\n",
        "\n",
        "You should see 7 `.jpg` images in the folder. Upload those to Colab prior to continuing, or feel free to grab your own images of cats and dogs!"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "### Use TF Keras API\n",
        "\n",
        "This requires the full tensorflow install. It uses the original model saved in a directory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 1s 582ms/step\n",
            "Pic: Vin.jpg\n",
            "prediction -9.335227\n",
            "Inferred label: 0\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "Pic: cat1.jpg\n",
            "prediction -9.482505\n",
            "Inferred label: 0\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "Pic: cat2.jpg\n",
            "prediction -11.236158\n",
            "Inferred label: 0\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "Pic: cat3.jpg\n",
            "prediction -6.9037724\n",
            "Inferred label: 0\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "Pic: dog1.jpg\n",
            "prediction 6.181109\n",
            "Inferred label: 1\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "Pic: dog2.jpg\n",
            "prediction 9.456733\n",
            "Inferred label: 1\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "Pic: dog3.jpg\n",
            "prediction 7.2035856\n",
            "Inferred label: 1\n"
          ]
        }
      ],
      "source": [
        "#use Keras API\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "model = tf.keras.models.load_model('cat-dog-tuned')\n",
        "\n",
        "for p in ['Vin.jpg', 'cat1.jpg', 'cat2.jpg', 'cat3.jpg', 'dog1.jpg', 'dog2.jpg', 'dog3.jpg']:\n",
        "    '''Labels\n",
        "        0 = Cat\n",
        "        1 = Dog    \n",
        "    '''\n",
        "    # Load and resize the image\n",
        "    img = tf.keras.utils.load_img(\n",
        "        p, target_size=(160, 160)\n",
        "    )\n",
        "    img_array = tf.keras.utils.img_to_array(img)\n",
        "    img_array = tf.expand_dims(img_array, 0) # Create a batch of size 1\n",
        "\n",
        "    # Conduct inference and extract the result from the np array\n",
        "    prediction = model.predict(img_array)\n",
        "    result = np.squeeze(prediction)\n",
        "    \n",
        "    # This is an exponential function used to gauge confidence\n",
        "    sig_result = tf.nn.sigmoid(result)\n",
        "    sig_predict = tf.where(sig_result < 0.5, 0, 1)\n",
        "    sig_predict = sig_predict.numpy()\n",
        "\n",
        "    print(\"Pic:\", p)\n",
        "    print(\"prediction\", result)\n",
        "    print(\"Inferred label:\", sig_predict)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Use TF Lite Interpreter\n",
        "\n",
        "This more closely mirrors what we'll do on our embedded system.\n",
        "The only difference is we will use the included `tf.lite` module instead of the standalone `tflite-runtime`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pic: Vin.jpg\n",
            "Prediction: -9.166648\n",
            "Inferred label: 0\n",
            "Pic: cat1.jpg\n",
            "Prediction: -8.077365\n",
            "Inferred label: 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pic: cat2.jpg\n",
            "Prediction: -7.1449704\n",
            "Inferred label: 0\n",
            "Pic: cat3.jpg\n",
            "Prediction: -5.9414144\n",
            "Inferred label: 0\n",
            "Pic: dog1.jpg\n",
            "Prediction: 5.4305377\n",
            "Inferred label: 1\n",
            "Pic: dog2.jpg\n",
            "Prediction: 10.539045\n",
            "Inferred label: 1\n",
            "Pic: dog3.jpg\n",
            "Prediction: 7.9097714\n",
            "Inferred label: 1\n"
          ]
        }
      ],
      "source": [
        "# Use tf.lite interpreter\n",
        "import tensorflow as tf # on embedded device use: import tflite_runtime.interpreter as tflite\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "model_path = \"cat-dog.tflite\"\n",
        "\n",
        "# For running on tflite-runtime replace this with tflite.Interpreter\n",
        "interpreter = tf.lite.Interpreter(model_path=model_path)\n",
        "\n",
        "# Embedded devices are memory constrained, so this handles that\n",
        "interpreter.allocate_tensors()\n",
        "# Details about model inputs and outputs\n",
        "input_details = interpreter.get_input_details()\n",
        "output_details = interpreter.get_output_details()\n",
        "input_shape = input_details[0][\"shape\"]\n",
        "\n",
        "for p in ['Vin.jpg', 'cat1.jpg', 'cat2.jpg', 'cat3.jpg', 'dog1.jpg', 'dog2.jpg', 'dog3.jpg']:\n",
        "    '''Labels\n",
        "        0 = Cat\n",
        "        1 = Dog    \n",
        "    '''\n",
        "    # Load the image using PIL\n",
        "    image = Image.open(p)\n",
        "    # Resize the image to match what the model was trained on\n",
        "    resized_image = image.resize((input_shape[1], input_shape[2]))\n",
        "    input_data = np.array(resized_image, dtype=np.float32)\n",
        "    input_data = np.expand_dims(input_data, axis=0) # Create a batch of size 1\n",
        "\n",
        "    # Conduct inference\n",
        "    interpreter.set_tensor(input_details[0][\"index\"], input_data)\n",
        "    interpreter.invoke()\n",
        "    output_data = interpreter.get_tensor(output_details[0][\"index\"])\n",
        "    # Pull out the raw value from the np array\n",
        "    prediction = np.squeeze(output_data)\n",
        "\n",
        "    # Computing exponents for sigmoid function is expensive, so use a simple heuristic instead.\n",
        "    # If  you need an \"unknown\" option or confidence threshold, use something like this.\n",
        "    # label = 0 if prediction < -3 else (1 if prediction > 3 else -1)\n",
        "    label = 0 if prediction < 0 else 1\n",
        "\n",
        "    print(\"Pic:\", p)\n",
        "    print(\"Prediction:\", prediction)\n",
        "    print(\"Inferred label:\", label)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Next step: embedded device!\n",
        "\n",
        "Now go put the tflite model on an embedded device, such as Raspberry Pi or Arduino and conduct inference!\n",
        "\n",
        "### Running on Raspberry Pi OS Bullseye 11\n",
        "\n",
        "You can get started with:\n",
        "\n",
        "```bash\n",
        "pip install tflite-runtime==2.11.0\n",
        "```\n",
        "\n",
        "Then change the above code to use `tflite` instead of `tf.lite`, as annotated in the comments.\n",
        "\n",
        "Finally, run it! Then considering using your picamera for live inference!"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "transfer_learning.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
