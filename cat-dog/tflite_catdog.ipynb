{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "dfec"
      },
      "source": [
        "##### Copyright 2023 Brian Yarbrough."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Tensorflow Lite\n",
        "\n",
        "Previously, we trained a model based on MobileNetV2 to differentiate between cats and dogs.\n",
        "\n",
        "Prior to conducting inference with this model, we will convert the model to Tensorflow Lite.\n",
        "This will have performance advantages on constrained hardware.\n",
        "\n",
        "## Upload the Model\n",
        "\n",
        "First, upload the saved model (`cat-dog-tuned.zip` if you used the previous tutorial).\n",
        "Then unzip the model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run after uploading the file\n",
        "!unzip cat-dog-tuned.zip"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Convert the saved model\n",
        "\n",
        "We'll use [TFLiteConverter](https://www.tensorflow.org/lite/api_docs/python/tf/lite/TFLiteConverter) to export the model to a single binary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pT8t5BDynwh_",
        "outputId": "0a1e1b26-8e20-401a-ae47-dfec0979c374"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "print(help(tf.lite.TFLiteConverter))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To convert the model we will open it and then follow [the docs](https://www.tensorflow.org/lite/models/convert/convert_models#convert_a_savedmodel_recommended_)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "saved_model_dir = \"cat-dog-tuned\"  # path to the SavedModel directory\n",
        "tflite_model = \"cat-dog.tflite\" # what to save the converted model as\n",
        "\n",
        "# Open the model\n",
        "converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)\n",
        "# Use Dynamic Range Quantization\n",
        "# https://www.tensorflow.org/lite/performance/post_training_quantization\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "# Convert the model.\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "# Save the model.\n",
        "with open('model.tflite', 'wb') as f:\n",
        "  f.write(tflite_model)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conduct inference on novel images\n",
        "\n",
        "Now that we've converted the model, let's test it with novel images!\n",
        "We can do this both with the full Keras API, or rely on the Tensorflow Lite runtime - based on our hardware and OS.\n",
        "\n",
        "#### Upload images\n",
        "\n",
        "You should see 7 `.jpg` images in the folder. Upload those to Colab prior to continuing, or feel free to grab your own images of cats and dogs!"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "### Use TF Keras API\n",
        "\n",
        "This requires the full tensorflow install. It uses the original model saved in a directory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#use Keras API\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "model = tf.keras.models.load_model('cat-dog-tuned')\n",
        "\n",
        "for p in ['Vin.jpg', 'cat1.jpg', 'cat2.jpg', 'cat3.jpg', 'dog1.jpg', 'dog2.jpg', 'dog3.jpg']:\n",
        "    '''Labels\n",
        "        0 = Cat\n",
        "        1 = Dog    \n",
        "    '''\n",
        "    # Load and resize the image\n",
        "    img = tf.keras.utils.load_img(\n",
        "        p, target_size=(160, 160)\n",
        "    )\n",
        "    img_array = tf.keras.utils.img_to_array(img)\n",
        "    img_array = tf.expand_dims(img_array, 0) # Create a batch of size 1\n",
        "\n",
        "    # Conduct inference and extract the result from the np array\n",
        "    prediction = model.predict(img_array)\n",
        "    result = np.squeeze(prediction)\n",
        "    \n",
        "    # This is an exponential function used to gauge confidence\n",
        "    sig_result = tf.nn.sigmoid(result)\n",
        "    sig_predict = tf.where(sig_result < 0.5, 0, 1)\n",
        "    sig_predict = sig_predict.numpy()\n",
        "\n",
        "    print(\"Pic:\", p)\n",
        "    print(\"prediction\", result)\n",
        "    print(\"Inferred label:\", sig_predict)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Use TF Lite Interpreter\n",
        "\n",
        "This more closely mirrors what we'll do on our embedded system.\n",
        "The only difference is we will use the included `tf.lite` module instead of the standalone `tflite-runtime`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Use tf.lite interpreter\n",
        "import tensorflow as tf # on embedded device use: import tflite_runtime.interpreter as tflite\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "model_path = \"cat-dog.tflite\"\n",
        "\n",
        "# For running on tflite-runtime replace this with tflite.Interpreter\n",
        "interpreter = tf.lite.Interpreter(model_path=model_path)\n",
        "\n",
        "# Embedded devices are memory constrained, so this handles that\n",
        "interpreter.allocate_tensors()\n",
        "# Details about model inputs and outputs\n",
        "input_details = interpreter.get_input_details()\n",
        "output_details = interpreter.get_output_details()\n",
        "input_shape = input_details[0][\"shape\"]\n",
        "\n",
        "for p in ['Vin.jpg', 'cat1.jpg', 'cat2.jpg', 'cat3.jpg', 'dog1.jpg', 'dog2.jpg', 'dog3.jpg']:\n",
        "    '''Labels\n",
        "        0 = Cat\n",
        "        1 = Dog    \n",
        "    '''\n",
        "    # Load the image using PIL\n",
        "    image = Image.open(p)\n",
        "    # Resize the image to match what the model was trained on\n",
        "    resized_image = image.resize((input_shape[1], input_shape[2]))\n",
        "    input_data = np.array(resized_image, dtype=np.float32)\n",
        "    input_data = np.expand_dims(input_data, axis=0) # Create a batch of size 1\n",
        "\n",
        "    # Conduct inference\n",
        "    interpreter.set_tensor(input_details[0][\"index\"], input_data)\n",
        "    interpreter.invoke()\n",
        "    output_data = interpreter.get_tensor(output_details[0][\"index\"])\n",
        "    # Pull out the raw value from the np array\n",
        "    prediction = np.squeeze(output_data)\n",
        "\n",
        "    # Computing exponents for sigmoid function is expensive, so use a simple heuristic instead.\n",
        "    # If  you need an \"unknown\" option or confidence threshold, use something like this.\n",
        "    # label = 0 if prediction < -3 else (1 if prediction > 3 else -1)\n",
        "    label = 0 if prediction < 0 else 1\n",
        "\n",
        "    print(\"Pic:\", p)\n",
        "    print(\"Prediction:\", prediction)\n",
        "    print(\"Inferred label:\", label)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Next step: embedded device!\n",
        "\n",
        "Now go put the tflite model on an embedded device, such as Raspberry Pi or Arduino and conduct inference!\n",
        "\n",
        "### Running on Raspberry Pi OS Bullseye 11\n",
        "\n",
        "You can get started with:\n",
        "\n",
        "```bash\n",
        "pip install tflite-runtime==2.11.0\n",
        "```\n",
        "\n",
        "Then change the above code to use `tflite` instead of `tf.lite`, as annotated in the comments.\n",
        "\n",
        "Finally, run it! Then considering using your picamera for live inference!"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "transfer_learning.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
